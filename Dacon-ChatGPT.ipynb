{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "sample_file = 'data/sample.csv'\n",
    "\n",
    "# train_df = pd.read_csv(train_file)\n",
    "# test_df = pd.read_csv(test_file)\n",
    "# sample_df = pd.read_csv(sample_file)\n",
    "\n",
    "# train_df['text'] = train_df['text'].apply(str)\n",
    "# test_df['text'] = test_df['text'].apply(str)\n",
    "# sample_df = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, df, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = df\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    text = self.data['text'][index]\n",
    "    label = self.data['label'][index] if 'label' in self.data.columns else None\n",
    "\n",
    "    # Tokenize the text\n",
    "    encoded_dict = self.tokenizer.encode_plus(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "    input_ids = encoded_dict['input_ids'][0]\n",
    "    attention_mask = encoded_dict['attention_mask'][0]\n",
    "    labels = encoded_dict['labels'] if 'labels' in encoded_dict else None\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomDataLoader(train_file, test_file, tokenizer, batch_size, valid_ratio):\n",
    "    # load train data\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle rows\n",
    "    \n",
    "    # split train data into train and validation sets\n",
    "    num_train = int(len(train_df) * (1 - valid_ratio))\n",
    "    train_set, valid_set = random_split(train_df, [num_train, len(train_df) - num_train])\n",
    "    \n",
    "    # load test data\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # create data loaders\n",
    "    train_loader = DataLoader(CustomDataset(train_set, tokenizer), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    valid_loader = DataLoader(CustomDataset(valid_set, tokenizer), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(CustomDataset(test_df, tokenizer), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "batch_size = 16\n",
    "valid_ratio = 0.2\n",
    "\n",
    "train_loader, valid_loader, test_loader = CustomDataLoader(train_file, test_file, tokenizer, batch_size, valid_ratio)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
